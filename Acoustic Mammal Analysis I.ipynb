{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a34c7f",
   "metadata": {},
   "source": [
    "# Detecting Marine Mammal Calls in MBARI Hydrophone Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d2d58",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "Artash Nath\n",
    "Founder, MonitorMyOcean.com\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00039fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing open-source libraries\n",
    "\n",
    "# AWS Client Data Libraries\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from numba import jit\n",
    "\n",
    "# Standard Libaries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import Audio\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Statistics / Analsyis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.patches import Rectangle\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import librosa\n",
    "from scipy.signal import find_peaks\n",
    "import sklearn\n",
    "from scipy.ndimage import convolve\n",
    "import scipy\n",
    "\n",
    "# Custom Helper Functions\n",
    "\n",
    "from helper_functions import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936761f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878ac74",
   "metadata": {},
   "source": [
    "----------------\n",
    "Preliminary Functions and Constants\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1729b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue Whale Call Parameters (From MBARI)\n",
    "\n",
    "BLUE_A = dict(name = 'BlueA',\n",
    "    freq_range=[(70,90)], duration_secs=25, blur_axis='frequency', num_fft=1024, center=True,  padding_secs=3, num_mels=30)\n",
    "\n",
    "BLUE_B = dict(name = 'BlueB',\n",
    "    freq_range=[(40,55)], duration_secs=25, blur_axis='time', num_fft=1024, center=True, padding_secs=2)\n",
    "\n",
    "BLUE_D = dict(name = 'BlueD',\n",
    "    freq_range=[(25,75)], duration_secs=7, blur_axis='', num_fft=1024, center=True, padding_secs=2, num_mels=30)\n",
    "\n",
    "FIN_20 = dict( \n",
    "    freq_range=[(10,35)], duration_secs=0, blur_axis='frequency', num_fft=4096, center=False,  padding_secs=3)\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000 # Hydrophone File Sample rate\n",
    "OVERLAP = 0.95 # Overlap when taking precise interst-sample FFTs\n",
    "IMAGE_SIZE = (224, 224) # Size of sample images being fed into AI models\n",
    "\n",
    "MBARI_HYDROPHONE_CALIBRATION = 177.9\n",
    "global MBARI_HYDROPHONE_CALIBRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164c539",
   "metadata": {},
   "source": [
    "----------------\n",
    "Downloading Raw Hydrophone Data at 16 kHz from AWS Server\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa27e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'E:\\MBARI DATA'\n",
    "\n",
    "# Timing parameters\n",
    "year = 2017\n",
    "month = 8\n",
    "bucket = 'pacific-sound-16khz'\n",
    "\n",
    "# Client to retreive AWS MBARI Raw Acoustic Data\n",
    "s3 = boto3.client('s3',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "filenames = []\n",
    "for obj in s3.list_objects_v2(Bucket=bucket, Prefix=f'{year:04d}/{month:02d}')['Contents']:\n",
    "    filenames.append(obj['Key'])\n",
    "    \n",
    "s3 = boto3.resource('s3',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# First day in the month    \n",
    "n = filenames[0][8:]\n",
    "wav_filename_full = os.path.join(data_path, n)\n",
    "wav_filename = n\n",
    "key = f'{year:04d}/{month:02d}/{wav_filename}'\n",
    " \n",
    "# only download if needed\n",
    "if not Path(wav_filename_full).exists():\n",
    "    \n",
    "    print('Downloading') \n",
    "    s3.Bucket(bucket).download_file(key, wav_filename_full)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6c0c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Labels ['baf', 'bat']\n",
      "Training image mean: [0.18747011 0.66276884 0.68202728]\n",
      "Training image std: [0.03253593 0.01769102 0.01564376]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Blue Whale A-Call Model\n",
    "\n",
    "model_blue_A = tf.keras.models.load_model(r\"C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\ML Models\\bluewhale-a-resnet\")\n",
    "\n",
    "config_blue_A = json.load(open(r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\ML Models\\bluewhale-a-resnet\\config.json'))\n",
    "image_mean_blue_A = np.asarray(config_blue_A[\"image_mean\"])\n",
    "image_std_blue_A = np.asarray(config_blue_A[\"image_std\"])\n",
    "print(f\"Labels {config_blue_A['classes']}\")\n",
    "print(f\"Training image mean: {image_mean_blue_A}\")\n",
    "print(f\"Training image std: {image_std_blue_A}\")\n",
    "\n",
    "global model_blue_A\n",
    "global config_blue_A\n",
    "global image_mean_blue_A\n",
    "global image_std_blue_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2977ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\oceansoundscape2\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:327: UserWarning: front_end is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\oceansoundscape2\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:327: UserWarning: humpback_model is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "metadata:\n",
      "  class_names: [b'Mn']\n",
      "  context_width_samples: 39124\n",
      "  input_sample_rate: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Humpback Call model and get its score_fn for use in analyzing waveform data at 10kHz:\n",
    "\n",
    "model_humphback = tf.keras.models.load_model(\"./ML Models/humphback1\")\n",
    "\n",
    "model_humphback_score = model_humphback.signatures['score']\n",
    "\n",
    "metadata_fn_humphback = model_humphback.signatures['metadata']\n",
    "metadata_humphback = metadata_fn_humphback()\n",
    "print('metadata:')\n",
    "for k, v in metadata_humphback.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "global metadata_fn_humphback\n",
    "global model_humphback_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e26a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multipilot = tf.keras.models.load_model('saved_model/multimodel-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609214b4",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "Spectrum Analysis Functions\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef6a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram(x, sample_rate, num_fft, OVERLAP, freq_range, duration, mammal):\n",
    "    \n",
    "    if mammal == 'blue': \n",
    "        n_mels = 30\n",
    "    elif mammal == 'humphback': \n",
    "        n_mels = 30\n",
    "    # Optimum mel spectrogram parameters for classification\n",
    "    PCEN_GAIN = 0.25\n",
    "    PCEN_BIAS = 2.0\n",
    "    PCEN_TIME_CONSTANT = 0.6\n",
    "    \n",
    "    hop_length = round(num_fft * (1 - OVERLAP))\n",
    "    \n",
    "    stft = librosa.feature.melspectrogram(\n",
    "                            y=sklearn.preprocessing.minmax_scale(x, feature_range=((-2 ** 31), (2 ** 31))),\n",
    "                            sr=sample_rate,\n",
    "                            hop_length=hop_length,\n",
    "                            power=1,\n",
    "                            n_mels=n_mels,\n",
    "                            fmin=freq_range[0],\n",
    "                            fmax=freq_range[1])\n",
    "    \n",
    "    stft_pcen = librosa.pcen(stft * (2 ** 31), sr=sample_rate,\n",
    "                                                 hop_length=hop_length,\n",
    "                                                 gain=PCEN_GAIN, bias = PCEN_BIAS,\n",
    "                                                 time_constant=PCEN_TIME_CONSTANT)\n",
    "    \n",
    "    stft_pcen = smooth(stft_pcen, 'frequency')\n",
    "    \n",
    "    if mammal == 'blue':\n",
    "        start = int((duration*2000) / hop_length)\n",
    "        end = int(2 * (duration*2000) / hop_length)\n",
    "        #plt.imshow(stft_pcen[:, start:end], origin='lower')\n",
    "        plt.show()\n",
    "        im = colorizeDenoise(stft_pcen[:, start:end])[:,:,:3]\n",
    "        \n",
    "    elif mammal == 'humphback':\n",
    "        #plt.imshow(stft_pcen, origin='lower')\n",
    "        #plt.show()\n",
    "        im = colorizeDenoise(stft_pcen)[:,:,:3]\n",
    "    return im, np.percentile(stft_pcen, 50), np.percentile(stft_pcen, 75), np.percentile(stft_pcen, 90)\n",
    "\n",
    "\n",
    "\n",
    "def mid_freq(a):\n",
    "    \n",
    "    weights = np.mean(a[:, :, 0], axis=1)\n",
    "    weights = weights/np.sum(weights)\n",
    "    weighted = []\n",
    "    for c, i in enumerate(weights):\n",
    "        weighted.append(i*c)\n",
    "    return np.sum(weighted)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def predict_a(im):\n",
    "    image_float = np.asarray(im).astype('float32')\n",
    "    image_float = image_float / 255.\n",
    "    image_float = (image_float - image_mean_blue_A) / ( image_std_blue_A + 1.e-9)\n",
    "    image = np.concatenate([image_float[np.newaxis, :, :]] * 1)\n",
    "    tensor_out = model_blue_A(image)\n",
    "    pred = tensor_out[0,1]\n",
    "    return float(np.array(pred))\n",
    "\n",
    "\n",
    "def BLUE_Analysis(x_2k, hour, date):\n",
    "    \n",
    "    annotations_A = []\n",
    "    annotations_B = []\n",
    "    \n",
    "    sg, f = psd_1sec(x_2k, 2000, MBARI_HYDROPHONE_CALIBRATION) # create calibrated psd\n",
    "    \n",
    "    windows_A = find_windows(sg, BLUE_A)\n",
    "    windows_B = find_windows(sg, BLUE_B)\n",
    "    print(\"{} Area-Of-Interests (A-Calls) were found in hour {}\".format(len(windows_A), hour))\n",
    "    print(\"{} Area-Of-Interests (B-Calls) were found in hour {}\".format(len(windows_B), hour))\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\{}'.format(date))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    directory_A = r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\{}\\Blue_A'.format(date)\n",
    "    directory_B = r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\{}\\Blue_B'.format(date)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(directory_A)\n",
    "        os.mkdir(directory_B)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for t1, t2, f1, f2 in tqdm(windows_A):\n",
    "        \n",
    "        try:\n",
    "            sample_width = BLUE_A['duration_secs']*2000\n",
    "            subset = x_2k[(t1*2000)-sample_width:(t2*2000)+sample_width]\n",
    "            im, p50, p75, p90 = mel_spectrogram(subset, 2000, BLUE_A['num_fft'], OVERLAP, [70, 100], BLUE_A['duration_secs'], 'blue')\n",
    "            p = round(predict_a(im), 3)\n",
    "            if p > 0.6:\n",
    "                mid_f = mid_freq(im)\n",
    "                mid_f = (f2-f1)*(mid_f/224)\n",
    "                mid_f+=f1\n",
    "                annotations_A.append([date, hour, t1, 'BLUE_A', p, sample_width, mid_f, p50, p75, p90])\n",
    "                name = f'{date}T{hour}-{t1}-BlueA'\n",
    "                plt.imsave(os.path.join(directory_A, name+'.png'), im, origin='lower')\n",
    "                \n",
    "            if p < 0.05:\n",
    "                name = f'{date}T{hour}-{t1}-NoiseA'\n",
    "                plt.imsave(os.path.join(r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\Noise', name+'.png'), im, origin='lower')\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#################################################################################        \n",
    "        \n",
    "    for t1, t2, f1, f2 in tqdm(windows_B):\n",
    "        \n",
    "        try:\n",
    "            sample_width = BLUE_B['duration_secs']*2000\n",
    "            subset = x_2k[(t1*2000)-sample_width:(t2*2000)+sample_width]\n",
    "            \n",
    "            im, p50, p75, p90 = mel_spectrogram(subset, 2000, BLUE_B['num_fft'], OVERLAP, [25, 75], BLUE_B['duration_secs'], 'blue')\n",
    "            p = 1\n",
    "            mid_f = mid_freq(im)\n",
    "            mid_f = (f2-f1)*(mid_f/224)\n",
    "            mid_f+=f1\n",
    "            if (40<mid_f<50):\n",
    "                \n",
    "                annotations_B.append([date, hour, t1, 'BLUE_B', p, sample_width, mid_f, p50, p75, p90])\n",
    "                name = f'{date}T{hour}-{t1}-BlueB'\n",
    "                plt.imsave(os.path.join(directory_B, name+'.png'), im, origin='lower')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    print(\"{} Confirmed Blue-A calls in hour {}\".format(len(annotations_A), hour))\n",
    "    print(\"{} Possible Blue-B calls in hour {}\".format(len(annotations_B), hour))\n",
    "    return annotations_A + annotations_B\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def predict_humphback(psound_segment_at_10k):\n",
    "    \n",
    "    context_step_samples = tf.cast(10000, tf.int64)\n",
    "    waveform1 = np.expand_dims(psound_segment_at_10k, axis=1)\n",
    "    waveform_exp = tf.expand_dims(waveform1, 0)\n",
    "    psound_scores = model_humphback_score(waveform=waveform_exp, context_step_samples=context_step_samples)\n",
    "    score_values = psound_scores['scores'].numpy()[0]\n",
    "    return score_values\n",
    "\n",
    "def humphback_analysis(x_10k, x_2k, hour, date):\n",
    "    \n",
    "    annotations = []\n",
    "\n",
    "    x_brk = np.split(x_10k, 72)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\{}'.format(date))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    directory = r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\{}\\Humphback'.format(date)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i, elem in enumerate(x_brk):\n",
    "        mean_score = np.mean(predict_humphback(elem))\n",
    "        if mean_score > 0.7:\n",
    "            \n",
    "            subset_2k = x_2k[i*50*2000:(i+1)*50*2000]\n",
    "            im, p50, p75, p90 = mel_spectrogram(subset_2k, 2000, 1024, OVERLAP, [100, 1000], 50, 'humphback')\n",
    "            mid_f = mid_freq(im)\n",
    "            mid_f = 900*(mid_f/224)\n",
    "            mid_f+=100\n",
    "            name = f'{date}T{hour}-{(i*50)}-Humphback'\n",
    "            \n",
    "            plt.imsave(os.path.join(directory, name+'.png'), im, origin='lower')\n",
    "            annotations.append([date, hour, (i*50), 'Humpback', mean_score, 50, mid_f, p50, p75, p90])\n",
    "            \n",
    "        if mean_score < 0.07:\n",
    "            \n",
    "            subset_2k = x_2k[i*50*2000:(i+1)*50*2000]\n",
    "            im, p50, p75, p90 = mel_spectrogram(subset_2k, 2000, 1024, OVERLAP, [100, 1000], 50, 'humphback')\n",
    "            name = f'{date}T{hour}-{(i*50)}-NoiseH'\n",
    "            \n",
    "            plt.imsave(os.path.join(r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations\\Noise', name+'.png'), im, origin='lower')\n",
    "            \n",
    "            \n",
    "    print(\"{} Confirmed Humpback calls in hour {}\".format(len(annotations), hour))\n",
    "    return annotations\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d0f48",
   "metadata": {},
   "source": [
    "----------------\n",
    "Analyzing Specified Day Hour by Hour\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb7b67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - BLUE WHALE ANALYSIS (A Calls, B Calls)\n",
      "12 Area-Of-Interests (A-Calls) were found in hour 0\n",
      "22 Area-Of-Interests (B-Calls) were found in hour 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc28d1e1258448ca568304846bd1b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2dcf715c74a2d921efcb3d321df0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Confirmed Blue-A calls in hour 0\n",
      "22 Possible Blue-B calls in hour 0\n",
      "\n",
      "2 - HUMPHBACK WHALE ANALYSIS (Songs)\n",
      "0 Confirmed Humpback calls in hour 0\n",
      "\n",
      "1 - BLUE WHALE ANALYSIS (A Calls, B Calls)\n",
      "11 Area-Of-Interests (A-Calls) were found in hour 1\n",
      "16 Area-Of-Interests (B-Calls) were found in hour 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfabc264cb5e48a08b340e7d5a192c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565020b2655947399a366b01f6ee4d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Confirmed Blue-A calls in hour 1\n",
      "16 Possible Blue-B calls in hour 1\n",
      "\n",
      "2 - HUMPHBACK WHALE ANALYSIS (Songs)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for start_hour in range(0, 24):\n",
    "    \n",
    "    date = wav_filename.split('-')[1].split('T')[0]\n",
    "    \n",
    "    start_frame = int(SAMPLE_RATE * start_hour * 3600)\n",
    "    duration_frames =  int(SAMPLE_RATE* 3600)\n",
    "\n",
    "    pacsound_file = sf.SoundFile(wav_filename_full)\n",
    "    pacsound_file.seek(start_frame)\n",
    "    x = pacsound_file.read(duration_frames, dtype='float32')\n",
    "    \n",
    "    print(\"1 - BLUE WHALE ANALYSIS (A Calls, B Calls)\")\n",
    "    x_2k = scipy.signal.resample(x, (2000*3600))\n",
    "    x_10k = scipy.signal.resample(x, (10000*3600))\n",
    "    \n",
    "    l = BLUE_Analysis(x_2k, start_hour, date)\n",
    "    labels += l\n",
    "    print()\n",
    "    print(\"2 - HUMPHBACK WHALE ANALYSIS (Songs)\")\n",
    "    l = humphback_analysis(x_10k, x_2k, start_hour, date)\n",
    "    labels+=l\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9a8824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84be0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(labels, columns= ['Date', 'Hour','Second', 'Label', 'Confidence', 'Duration', 'Mean Frequency', 'L50 Power', 'L75 Power', 'L90 Power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b6f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(os.path.join(r'C:\\Users\\vikas\\OneDrive\\Artash_Python\\2023 Test\\1 - Whale Detection\\Annotations', date+'.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc134710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
